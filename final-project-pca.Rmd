---
title: "Final Project Write-Up"
author: "Alex Xiaotong Gui"
date: "5/4/2018"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, fig.height=3, fig.width=8, fig.align = "center",tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(dplyr)
library(ggplot2)
library(skimr)
library(broom)
library(readr)
library(gridExtra)
library(PerformanceAnalytics)
library(jtools)
library(GGally)
library(grid)
options(digits=3)
pokemon <- read_csv("pokemon.csv")
```


\subsection{Unsupervised Learning}
In the previous write-ups, our analysis concerns predictions: given a set of explanatory variables $X_1,...X_{p-1}$ and $n$ observations, how can we predict the response variable $Y$? We have adopted various regression techniques and yielded satisfying results. However, in this part of analysis, we want to shift our focus away from the $Y$ variable and ask ourselves: what are something interesting things we can say about the $X$s? What are some underlying structures and how can we present them efficiently? We introduce the concept of unsupervised learning, "a set of statistical tools intended for the setting in which we have only a set of features X1, X2, . . . , Xp measured on n observations."

\subsection{Principal Components Analysis(PCA)}
Principal components analysis is widely used as an unsupervised learning method for feature extraction and data compression. In our analysis, we will apply principal components in our regression model as a dimensionality reduction technique. The intuition behind PCA is: given a set of highly correlated predictors, PCA will transform it into a smaller set of linearly independent variables called principal components. The transformation is defined such that the first principal component direction captures the greatest possible variability in the data, in others words, explain the most variability of the data. The succeeding principal components are linear combinations of the variables that is un- correlated with the preceding component and has largest variance subject to this constraint. The set of components constitute a basis for our data space.

\subsection{Principal Components Regression}
```{r,echo=FALSE}
pokemondata<-pokemon %>%
  dplyr::select(-notes,-name,-attack_weak_type,-attack_strong_type,-attack_strong_type_new,-attack_weak,-attack_strong,-attack_weak_type_new,-attack_weak_new,-attack_strong_new) %>%
  mutate(species=ifelse(species=="Pidgey",1,ifelse(species=="Weedle",2,ifelse(species=="Caterpie",3,4)))) %>%
  mutate(cpnew=cp_new)

pokemondata<-pokemondata[,!grepl("_new", colnames(pokemondata))]
```


The principal components regression approach will first construct $M$ principal components and then regress on the components instead of individual predictors. The underlying assumption of the model is "the directions in which $X_1,...X_p$ shows the greatest variance are those associated with Y". Although this assumption is not guaranteed, it regardless provides a decent approximation that often yields good results. $M$, the number of principal components, is our tuning parameter that will be chosen by cross-validation.

We believe PCA works well with our pokemon data given the existence of strong correlation among our predictors. Our analysis shows that PCR greatly reduced variance and contributed to our model's predictive power.

```{r,fig.width=8,fig.height=5,include=FALSE}
res<-cor(pokemondata)
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
title(sub="Correlation Plot")
```


## Principal Components
Our model first constructs 9 principal components(this makes sense since $p=9$ and $M \leq p$). 
```{r,echo=FALSE}
pokemonpca<-pokemondata[,!grepl("new", colnames(pokemondata))]

pca <- prcomp(pokemonpca,scale. = TRUE)
pca
```

To visualize it:

```{r,echo=FALSE,fig.width=5,fig.height=5,echo=FALSE}
biplot(pca,scale=1)
```

Look at PC1. We observe that the higher the performance metrics, the higher the PC1 value. Therefore we can interpret PC1 as a measurement of overall strength. As for PC2, we notice that higher PC2 is associated with higher attack value. Therefore we interpret PC2 as a measurement of attack strength.

We then plot our pokemon on our new principal components space:

```{r,fig.width=5,fig.height=5,echo=FALSE}
require(png)

# Get the PCA data
pd <- cbind.data.frame(pokemondata, pca$x)

#change back to specie names
pd$species <- as.character(pd$species)
pd$species[pd$species == "1"] <- "Pidgey"
pd$species[pd$species == "2"] <- "Weedle"
pd$species[pd$species == "3"] <- "Caterpie"
pd$species[pd$species == "4"] <- "Eevee"

# A function to plot Pokemon's png file as ggplot2's annotation_custom
f_annotate <- function(x, y, name, size) {
  f_getImage <- function(name) {
    rasterGrob(readPNG(paste0("pokemon_png/", name, ".png")))
  }
  annotation_custom(f_getImage(name),
                    xmin = x - size, xmax = x + size, 
                    ymin = y - size, ymax = y + size)
}

# Wrap everything in a plot function
f_plot <- function(pd) {
  ggplot(data = pd, aes(x = PC1, y = PC2)) +
    geom_text(data = pd, aes(label = species), 
              hjust = 0.5, vjust = -1, size = 1.5, alpha = 0.5) +
    mapply(f_annotate, x = pd$PC1, y = pd$PC2, name = pd$species, size = .5)  +
    theme_bw() +
    labs(x = "Overall Strength (PC1)", y = "Attack Strength (PC2)") +
    coord_fixed(xlim = c(-5, 5), ylim = c(-4, 4))
}


f_plot(pd)
```

\textbf{Interesting Insights:} Using principal components, we get to create two new powerful metrics to evaluate our pokemons. From the plot, you can observe that Eeeve in general has high overall strength and high attack strength. Pidgey has good attack stats but is weaker in general due to the creature's intrinsic nature. Caterpie and Weedle are weak on both metrics. Overall, this PCA gives you a high-level overview of our pokemon's strength. You should feel excited!

## Regression: 

```{r,echo=FALSE}
pr.var=pca$sdev ^2
pve=pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='l')
```
Observe that the first two principal components explain nearly 60% of the variability in the data. As $M \to 10$, the marginal contribution to variability explained decreases. Our regression model will use cross validation to tune $M$, the number of components as predictors.


```{r,echo=FALSE}
library(pls)
set.seed(47)
train=sample(1:nrow(pokemondata),nrow(pokemondata)*2/3)
pcr.fit = pcr(cpnew~.,data=pokemondata[train,],scale=TRUE,validation="CV")
summary(pcr.fit)
```


The metric of cross-validation is $root\_mean\_squared\_error$. Ideally, we want to pick $M$ where the CV score is minimized. 

```{r,echo=FALSE}
validationplot(pcr.fit,val.type="MSEP")
```

We compute the test MSE as follows using $M=9$ components:
```{r}
pcr.pred=predict(pcr.fit,pokemondata[-train,],ncomp=9) 
mean((pcr.pred-pokemondata[-train,]$cpnew)^2)
```


\subsubsection{Interpretation}

