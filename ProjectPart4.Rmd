---
title: "Part 4: Final Report"
author: "Alex Gui and Lathan Liou"
date: "Date Submitted: "
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, fig.height=3, fig.width=8, fig.align = "center",tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(dplyr)
library(ggplot2)
library(skimr)
library(broom)
library(readr)
library(gridExtra)
library(GGally)
library(glmnet)
library(tidyr)
library(splines)
library(grid)
library(png)
library(pls)
options(digits=3)
pokemon <- read_csv("pokemon.csv")

pokemon_model1<-pokemon %>%
  select(-name,-notes) %>%
  select(-hp_new,-power_up_stardust_new,-attack_strong_value_new,-attack_weak_value_new,-weight_new,-height_new,-power_up_candy_new,-attack_strong_type_new,-attack_weak_type_new,-attack_strong_new,-attack_weak_new) %>%
  mutate(attack_strong_type = as.factor(attack_strong_type),
         attack_weak_type = as.numeric(as.factor(attack_weak_type)),
         attack_strong = as.numeric(as.factor(attack_strong)),
         attack_weak = as.numeric(as.factor(attack_weak)),
         species = as.numeric(as.factor(species)))

pokemon_logged <- pokemon_model1 %>%
  mutate(cp_new=log(cp_new),cp=log(cp))
names(pokemon_logged)[2] <- c("cp_log")
names(pokemon_logged)[14] <- c("cp_new_log")

pokemon_test <- pokemon %>%
  select(-name,-notes) %>%
  select(-hp_new,-power_up_stardust_new,-attack_strong_value_new,-attack_weak_value_new,-weight_new,-height_new,-power_up_candy_new,-attack_strong_type_new,-attack_weak_type_new,-attack_strong_new,-attack_weak_new)

pokemon_testlog <- pokemon_test %>%
  mutate(cp_new=log(cp_new),cp=log(cp))
names(pokemon_testlog)[2] <- c("cp_log")
names(pokemon_testlog)[14] <- c("cp_new_log")
```

\section*{Introduction}
In July of 2016, Pokemon Go became an overnight sensation with hundreds of millions of people having downloaded the mobile game. The whole point of the game is to try to catch all the Pokemon available and train them (increase their combat power, which is abbreviated cp) so that you can battle other players with your strengthened Pokemon. A quick note about Pokemon is that they can evolve into stronger forms, so an evolved Pokemon will generally always have a higher cp than a non-evolved Pokemon. A number of people have tried to generate models in an attempt to predict the best way to maximize cp for their Pokemon. This is what we will attempt to do ourselves: create a model to predict combat power for an evolved Pokemon. 

To refresh your memory, the dataset we are looking at is an original data set collected by $OpenIntro^1$, most likely by some individual who was playing Pokemon Go and decided to record data. The dataset contains 75 observations across 26 variables, with each observation representing a randomly generated Pokemon that the gamer caught. Four species are represented in this data, so the conclusions drawn from this modeling process will reflect the population of these 4 particular species: Eevee, Pidgey, Caterpie, and Weedle. 

We avoid using "new" predictor variables in our modeling process because as a user, you wouldn't have access to any of the "new" information, but if you're interested in whether your pokemon will evolve into one with a high cp ($cp\_new$), you would want to know which of the Pokemon's current stats can indicate a high $cp\_new$. The variables that we are particularly interested in are cp, hp, and power_up_stardust. Our intuition is that a pokemon with a higher cp might evolve into a pokemon with a higher cp. Hp, or hit points, refers to the amount of damage a Pokemon can sustain in battle before fainting. It would be interesting to see whether a Pokemon with high hp will also have high cp. Power up stardust is used to raise cp of the pokemon, but the catch here is we don't know the ideal amount of power up stardust to max out the $cp\_new$ of the evolved pokemon. 

You can follow our work here: https://github.com/alexaaag/math158-project.

\section*{Ridge Regression and Lasso}
```{r, include=FALSE}
set.seed(312)

#create lambda grid
lambda.grid = 10^seq(5,-5, length =100)

#x matrix, categorical variables have been factorized and are numeric
x <- model.matrix( ~species + cp_log + hp + weight + height + power_up_stardust + power_up_candy + attack_weak + attack_weak_type + attack_weak_value + attack_strong + attack_strong_type + attack_strong_value, pokemon_logged)

#run CV RR
pokemon.rr.cv <- cv.glmnet(x, as.numeric(pokemon_logged$cp_new_log), alpha=0, lambda=lambda.grid, standardize = TRUE)
  
#find min lambda
opt_lambda_RR <- pokemon.rr.cv$lambda.min

#plot CVRR
plot(pokemon.rr.cv)
abline(v=log(pokemon.rr.cv$lambda.min), col="green")

#get RR coefficients
pokemon.rr.cv_fit <- pokemon.rr.cv$glmnet.fit
tidy(pokemon.rr.cv_fit)

#run CV lasso
pokemon.lasso.cv <- cv.glmnet(x, as.numeric(pokemon_logged$cp_new_log), alpha=1, lambda=lambda.grid, standardize = TRUE)

#find min lambda
opt_lambda_lasso <- pokemon.lasso.cv$lambda.min

#plot CVlasso
plot(pokemon.lasso.cv)
abline(v=log(pokemon.lasso.cv$lambda.min), col="green")

#get lasso coefficients
pokemon.lasso.cv_fit <- pokemon.lasso.cv$glmnet.fit
tidy(pokemon.lasso.cv_fit)

r2lasso <- pokemon.lasso.cv_fit$dev.ratio[which(pokemon.lasso.cv_fit$lambda == opt_lambda_lasso)]

MLR.model <- lm(cp_new_log ~ cp_log + species + attack_strong_value + 
    hp + weight, data = pokemon_logged)
summary(MLR.model)
```

```{r, echo=FALSE}
par(mfrow=c(1,2))
#plot CVRR
RRplot <- plot(pokemon.rr.cv)
abline(v=log(pokemon.rr.cv$lambda.min), col="green")

#plot CVlasso
lassoplot <- plot(pokemon.lasso.cv)
abline(v=log(pokemon.lasso.cv$lambda.min), col="green")
```
Ridge regression shrunk the coefficients closer to zero than multiple linear regression did. For instance, if we compare the coefficients of $cp_{log}$, it is 1.186 in our linear regression model, and 1.24e-5 in the ridge regression model. On the other hand, lasso regression not only shrunk the coefficients but also performed variable selection, selecting $cp_{log}$, $attack\_strong\_value$ and $attack\_weak\_value$ to name a few.

```{r, include=FALSE}
#predicted response from MLR
MLR.model<-lm(cp_new_log ~ cp_log + species + attack_strong_value + 
    hp + weight, data = pokemon_logged)
val.rr <- data.frame(pokemon_logged[,-10])
pred.MLR <- augment(MLR.model,newdata=val.rr,type.predict = "response")
pred.MLR$.fitted

#predicted response from RR
pred.RR <- predict(pokemon.rr.cv_fit, newx = x, s = opt_lambda_RR)

#predicted response from lasso
pred.lasso <- predict(pokemon.lasso.cv_fit, newx = x, s = opt_lambda_lasso)

#tidy data
pokemon_pred <- data.frame(pokemon_logged$cp_new_log, pred.MLR$.fitted, pred.RR, pred.lasso)
colnames(pokemon_pred) <- c("cp_new_obs", "cp_new_MLR", "cp_new_RR", "cp_new_lasso")
pokemon_tidy <- gather(pokemon_pred, key = Method, value = Prediction, -cp_new_obs)
```

```{r, echo=FALSE}
#plot predictions
ggplot(pokemon_tidy, aes(x = cp_new_obs, y = Prediction, group = Method, color = Method)) +
  geom_jitter() + 
  geom_smooth(method = 'lm', se = FALSE)
```

From this figure, it seems ridge regression and lasso seem to predict very similarly as multiple linear regression, since the slopes of each regression fit basically overlap each other. 

\section*{Smoothing}
```{r, include=FALSE}
xlims <- range(pokemon_logged$cp_log)
x.grid <- seq(from=xlims[1], to=xlims[2])

#regression spline with df=3
cpnew.rs1 <- lm(cp_new_log ~ bs(cp_log, df=3, degree = 1), data = pokemon_logged)
SSE.rs1 <- sum(cpnew.rs1$residuals^2)

cpnew.rs1.pred <- predict(cpnew.rs1, newdata = list(cp_log=x.grid), se = TRUE)
cpnew.rs1.se <- cbind(cpnew.rs1.pred$fit + 2*cpnew.rs1.pred$se.fit,
                   cpnew.rs1.pred$fit - 2*cpnew.rs1.pred$se.fit)

plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, 
     col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline (df=3), SSE=4.429", outer = F)
lines(x.grid, cpnew.rs1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs1.se, lwd = 1, col = "blue", lty = 3)

#regression spline with df=4
cpnew.rs2 <- lm(cp_new_log ~ bs(cp_log, df=4, degree = 1), data = pokemon_logged)
SSE.rs2 <- sum(cpnew.rs2$residuals^2)

cpnew.rs2.pred <- predict(cpnew.rs2, newdata = list(cp_log=x.grid), se = TRUE)
cpnew.rs2.se <- cbind(cpnew.rs2.pred$fit + 2*cpnew.rs2.pred$se.fit,
                   cpnew.rs2.pred$fit - 2*cpnew.rs2.pred$se.fit)

plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, 
     col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline (df=4), SSE=4.335", outer = F)
lines(x.grid, cpnew.rs2.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs2.se, lwd = 1, col = "blue", lty = 3)

#regression spline with df=5
cpnew.rs3 <- lm(cp_new_log ~ bs(cp_log, df=5, degree = 1), data = pokemon_logged)
SSE.rs3 <- sum(cpnew.rs3$residuals^2)

cpnew.rs3.pred <- predict(cpnew.rs3, newdata = list(cp_log=x.grid), se = TRUE)
cpnew.rs3.se <- cbind(cpnew.rs3.pred$fit + 2*cpnew.rs3.pred$se.fit,
                   cpnew.rs3.pred$fit - 2*cpnew.rs3.pred$se.fit)

plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, 
     col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline (df=5), SSE=4.300", outer = F)
lines(x.grid, cpnew.rs3.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs3.se, lwd = 1, col = "blue", lty = 3)

#regression spline with df=6
cpnew.rs4 <- lm(cp_new_log ~ bs(cp_log, df=6, degree = 1), data = pokemon_logged)
SSE.rs4 <- sum(cpnew.rs4$residuals^2)

cpnew.rs4.pred <- predict(cpnew.rs4, newdata = list(cp_log=x.grid), se = TRUE)
cpnew.rs4.se <- cbind(cpnew.rs4.pred$fit + 2*cpnew.rs4.pred$se.fit,
                   cpnew.rs4.pred$fit - 2*cpnew.rs4.pred$se.fit)

plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, 
     col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline (df=6), SSE=4.322", outer = F)
lines(x.grid, cpnew.rs4.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs4.se, lwd = 1, col = "blue", lty = 3)

#plot all the splines on one
splineplot <- plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline", outer = F)
lines(x.grid, cpnew.rs1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs1.se, lwd = 1, col = "blue", lty = 3)
lines(x.grid, cpnew.rs2.pred$fit, lwd = 2, col = "green")
matlines(x.grid, cpnew.rs2.se, lwd = 1, col = "green", lty = 3)
lines(x.grid, cpnew.rs3.pred$fit, lwd = 2, col = "red")
matlines(x.grid, cpnew.rs3.se, lwd = 1, col = "red", lty = 3)
lines(x.grid, cpnew.rs4.pred$fit, lwd = 2, col = "black")
matlines(x.grid, cpnew.rs4.se, lwd = 1, col = "black", lty = 3)
```

```{r, include=FALSE}
#loess, span 0.2
cpnew.loess1 <- loess(cp_new_log ~ cp_log, span = 0.2, data = pokemon_logged)

cpnew.loess1.pred <- predict(cpnew.loess1, newdata = data.frame(cp_log=x.grid), se = TRUE)
cpnew.loess1.se <- cbind(cpnew.loess1.pred$fit + 2*cpnew.loess1.pred$se.fit,
                    cpnew.loess1.pred$fit - 2*cpnew.loess1.pred$se.fit)
SSE.loess1 <- sum(cpnew.loess1$residuals^2)

#loess, span 0.4
cpnew.loess2 <- loess(cp_new_log ~ cp_log, span = 0.4, data = pokemon_logged)

cpnew.loess2.pred <- predict(cpnew.loess2, newdata = data.frame(cp_log=x.grid), se = TRUE)
cpnew.loess2.se <- cbind(cpnew.loess2.pred$fit + 2*cpnew.loess2.pred$se.fit,
                    cpnew.loess2.pred$fit - 2*cpnew.loess2.pred$se.fit)
SSE.loess2 <- sum(cpnew.loess2$residuals^2)

#loess, span 0.6
cpnew.loess3 <- loess(cp_new_log ~ cp_log, span = 0.6, data = pokemon_logged)

cpnew.loess3.pred <- predict(cpnew.loess3, newdata = data.frame(cp_log=x.grid), se = TRUE)
cpnew.loess3.se <- cbind(cpnew.loess3.pred$fit + 2*cpnew.loess3.pred$se.fit,
                    cpnew.loess3.pred$fit - 2*cpnew.loess3.pred$se.fit)
SSE.loess3 <- sum(cpnew.loess3$residuals^2)

#loess, span 0.8
cpnew.loess4 <- loess(cp_new_log ~ cp_log, span = 0.8, data = pokemon_logged)

cpnew.loess4.pred <- predict(cpnew.loess4, newdata = data.frame(cp_log=x.grid), se = TRUE)
cpnew.loess4.se <- cbind(cpnew.loess4.pred$fit + 2*cpnew.loess4.pred$se.fit,
                    cpnew.loess4.pred$fit - 2*cpnew.loess4.pred$se.fit)

#loess plot
loessplot <- plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = xlims, cex = .5, pch = 19, col = "darkgrey", xlab = "cp_log", ylab = "cp_new_log")
title("Loess Fit", outer = F)
lines(x.grid, cpnew.loess1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.loess1.se, lwd = 1, col = "blue", lty = 3)
lines(x.grid, cpnew.loess2.pred$fit, lwd = 2, col = "green")
matlines(x.grid, cpnew.loess2.se, lwd = 1, col = "green", lty = 3)
lines(x.grid, cpnew.loess3.pred$fit, lwd = 2, col = "red")
matlines(x.grid, cpnew.loess3.se, lwd = 1, col = "red", lty = 3)
lines(x.grid, cpnew.loess4.pred$fit, lwd = 2, col = "black")
matlines(x.grid, cpnew.loess4.se, lwd = 1, col = "black", lty = 3)
```

```{r, echo=FALSE}
par(mfrow=c(1,2))

splineplot <- plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = range(pokemon_logged$cp_log), cex = .5, pch = 19, col = "darkgrey", xlab = "Cp_log", ylab = "Cp_new_log")
title("Regression Spline", outer = F)
lines(x.grid, cpnew.rs1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.rs1.se, lwd = 1, col = "blue", lty = 3)
lines(x.grid, cpnew.rs2.pred$fit, lwd = 2, col = "green")
matlines(x.grid, cpnew.rs2.se, lwd = 1, col = "green", lty = 3)
lines(x.grid, cpnew.rs3.pred$fit, lwd = 2, col = "red")
matlines(x.grid, cpnew.rs3.se, lwd = 1, col = "red", lty = 3)
lines(x.grid, cpnew.rs4.pred$fit, lwd = 2, col = "black")
matlines(x.grid, cpnew.rs4.se, lwd = 1, col = "black", lty = 3)

loessplot <- plot(pokemon_logged$cp_log, pokemon_logged$cp_new_log, xlim = xlims, cex = .5, pch = 19, col = "darkgrey", xlab = "cp_log", ylab = "cp_new_log")
title("Loess Fit", outer = F)
lines(x.grid, cpnew.loess1.pred$fit, lwd = 2, col = "blue")
matlines(x.grid, cpnew.loess1.se, lwd = 1, col = "blue", lty = 3)
lines(x.grid, cpnew.loess2.pred$fit, lwd = 2, col = "green")
matlines(x.grid, cpnew.loess2.se, lwd = 1, col = "green", lty = 3)
lines(x.grid, cpnew.loess3.pred$fit, lwd = 2, col = "red")
matlines(x.grid, cpnew.loess3.se, lwd = 1, col = "red", lty = 3)
lines(x.grid, cpnew.loess4.pred$fit, lwd = 2, col = "black")
matlines(x.grid, cpnew.loess4.se, lwd = 1, col = "black", lty = 3)
```

Both the smoothing spline and the loess curves fit the data extremely well. Changing the degrees of freedom, and hence the number of knots, for the smoothing splines improves the fit minimally to a certain point past which increasing the degrees of freedom actually begins to increase SSE. Likewise, increasing the span from 0.2 for loess actually increases SSE. 

I would choose the regression spline model with 5 degrees of freedom because it fits the data very smoothly, and it still has a functional form which lends itself to interpretability. 

\subsection*{Conclusion}

Overall, running ridge regression, lasso, and smoothing methods such as regresion splines and loess did not improve the model fit relative to multiple linear regression by very much. This is fairly unsurprising to us because we noticed how extremely well linear regression fit our dataset previously, and this can most likely be attributed to the nature of how $cp_{new}$ is actually modeled in the game. We think $cp_{new}$ is likely coded into the game as a function of a linear combination of certain predictors and our multiple linear regression model fairly closely matches the real model used in-game. 

\section*{Something New}

\subsection*{Q-Q Plot}

A normal probability plot is used to identify substantial departures from normality in the data. We chose in particular to plot what is known as a normal quantile-quantile plot (Q-Q plot in short), which plots sample quantiles against theoretical quantiles from a continuous cumultaive distribution function such as the standard normal distribution. Mathematically, the Q–Q plot draws the $q$-th quantile of theoretical cumulative probability distribution function F against the q-th quantile of our sample data for a range of values of q. Thus, the Q–Q plot is a parametric curve indexed over [0,1] with values in the real plane $R^2$. The units of a Q-Q plot are rankits, which are the expected values of the order statistics of a sample from the standard normal distribution the same size as the data. A $y=x$ reference line is also plotted and if the sample data also come from a normal distribution, the points should fall roughly along this reference line. A q-q plot is important because it can provide more information about the nature of the departure from normality and assess "goodness-of-fit" graphically.

```{r, echo=FALSE}
pokemon.stdres = rstandard(MLR.model)
qqnorm(pokemon.stdres,
       ylab = "Standardized Residuals",
       xlab="Normal Scores",
       main="Pokemon cp_new")
qqline(pokemon.stdres)

qqnorm(pokemon_logged$cp_log)
qqline(pokemon_logged$cp_log, col = "steelblue", lwd = 2)
```

In our Q-Q plot, we see that the points do not fall on the line as they follow a nonlinear pattern, suggesting that $cp\_log$ does not follow a normal distribution. This is interesting to us because we see that even after we log-transformed cp, we see that there are still departures from normality, most likely caused by the Eevee outliers. 

\subsection*{Added Variable Plots}

Added variable plots, also known as partial regression plot, attempt to show the marginal effect of adding another variable to a model already having one or more independent variables. Added variable plots are formed by first computing the residuals of regressing the response variable against the independent variable(s) but omitting the variable of interest, $X_i$. Let's call this $Y._{[i]}$ Next, the residuals are computed from regressing $X_i$ against the remaining independent variables. Let's call this $X_{i.[i]}$ The residuals from $Y._{[i]}$ and $X_{i.[i]}$ are then plotted against each other. 
For this analysis, the underlying assumption is that variables aren't highly correlated. 

```{r, echo=FALSE}
library(car)
avPlot(MLR.model, variable = "weight", id.n = TRUE, main=paste("Added-Variable Plot: weight"))
```
This plot contains the least squares line which has slope of -0.01861, which suggests that the linear relationship between $cp\_new\_log$ and weight is slightly negative. The scatter of the points around the least square lines is about the same to the scatter around the horizontal line $e(cp\_new\_log|other \ variables)$, indicating that adding weight to the regression model does not substantially reduce the error sum of squares. In fact, the coefficient of partial determination for the linear effect of weight is $R^2_{Y weight|cp, species, attack\_strong\_value, hp}=0.0283$. 

Another benefit of an added variable plot is it allows us to determine influential points, after accounting for the other variables in the model.

\subsection{Unsupervised Learning}
In the previous write-ups, our analysis concerns predictions: given a set of explanatory variables $X_1,...X_{p-1}$ and $n$ observations, how can we predict the response variable $Y$? We have adopted various regression techniques and yielded satisfying results. However, in this part of analysis, we want to shift our focus away from the $Y$ variable and ask ourselves: what are something interesting things we can say about the $X$s? What are some underlying structures and how can we present them efficiently? We introduce the concept of unsupervised learning, "a set of statistical tools intended for the setting in which we have only a set of features X1, X2, . . . , Xp measured on n observations."

\subsection{Principal Components Analysis(PCA)}
Principal components analysis is widely used as an unsupervised learning method for feature extraction and data compression. In our analysis, we will apply principal components in our regression model as a dimensionality reduction technique. The intuition behind PCA is: given a set of highly correlated predictors, PCA will transform it into a smaller set of linearly independent variables called principal components. The transformation is defined such that the first principal component direction captures the greatest possible variability in the data, in others words, explain the most variability of the data. The succeeding principal components are linear combinations of the variables that is un- correlated with the preceding component and has largest variance subject to this constraint. The set of components constitute a basis for our data space.

\subsection{Principal Components Regression}
```{r,echo=FALSE}
pokemondata<-pokemon %>%
  dplyr::select(-notes,-name,-attack_weak_type,-attack_strong_type,-attack_strong_type_new,-attack_weak,-attack_strong,-attack_weak_type_new,-attack_weak_new,-attack_strong_new) %>%
  mutate(species=ifelse(species=="Pidgey",1,ifelse(species=="Weedle",2,ifelse(species=="Caterpie",3,4)))) %>%
  mutate(cpnew=cp_new)

pokemondata<-pokemondata[,!grepl("_new", colnames(pokemondata))]
```


The principal components regression approach will first construct $M$ principal components and then regress on the components instead of individual predictors. The underlying assumption of the model is "the directions in which $X_1,...X_p$ shows the greatest variance are those associated with Y". Although this assumption is not guaranteed, it regardless provides a decent approximation that often yields good results. $M$, the number of principal components, is our tuning parameter that will be chosen by cross-validation.

We believe PCA works well with our pokemon data given the existence of strong correlation among our predictors. Our analysis shows that PCR greatly reduced variance and contributed to our model's predictive power.

```{r,fig.width=8,fig.height=5,include=FALSE}
res<-cor(pokemondata)
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
title(sub="Correlation Plot")
```


## Principal Components
Our model first constructs 9 principal components(this makes sense since $p=9$ and $M \leq p$). 
```{r,echo=FALSE}
pokemonpca<-pokemondata[,!grepl("new", colnames(pokemondata))]

pca <- prcomp(pokemonpca,scale. = TRUE)
pca
```

To visualize it:

```{r,echo=FALSE,fig.width=5,fig.height=5,echo=FALSE}
biplot(pca,scale=1)
```

Look at PC1. We observe that the higher the performance metrics, the higher the PC1 value. Therefore we can interpret PC1 as a measurement of overall strength. As for PC2, we notice that higher PC2 is associated with higher attack value. Therefore we interpret PC2 as a measurement of attack strength.

We then plot our pokemon on our new principal components space:

```{r,fig.width=5,fig.height=5,echo=FALSE}
require(png)

# Get the PCA data
pd <- cbind.data.frame(pokemondata, pca$x)

#change back to specie names
pd$species <- as.character(pd$species)
pd$species[pd$species == "1"] <- "Pidgey"
pd$species[pd$species == "2"] <- "Weedle"
pd$species[pd$species == "3"] <- "Caterpie"
pd$species[pd$species == "4"] <- "Eevee"

# A function to plot Pokemon's png file as ggplot2's annotation_custom
f_annotate <- function(x, y, name, size) {
  f_getImage <- function(name) {
    rasterGrob(readPNG(paste0("pokemon_png/", name, ".png")))
  }
  annotation_custom(f_getImage(name),
                    xmin = x - size, xmax = x + size, 
                    ymin = y - size, ymax = y + size)
}

# Wrap everything in a plot function
f_plot <- function(pd) {
  ggplot(data = pd, aes(x = PC1, y = PC2)) +
    geom_text(data = pd, aes(label = species), 
              hjust = 0.5, vjust = -1, size = 1.5, alpha = 0.5) +
    mapply(f_annotate, x = pd$PC1, y = pd$PC2, name = pd$species, size = .5)  +
    theme_bw() +
    labs(x = "Overall Strength (PC1)", y = "Attack Strength (PC2)") +
    coord_fixed(xlim = c(-5, 5), ylim = c(-4, 4))
}


f_plot(pd)
```

\textbf{Interesting Insights:} Using principal components, we get to create two new powerful metrics to evaluate our pokemons. From the plot, you can observe that Eeeve in general has high overall strength and high attack strength. Pidgey has good attack stats but is weaker in general due to the creature's intrinsic nature. Caterpie and Weedle are weak on both metrics. Overall, this PCA gives you a high-level overview of our pokemon's strength. You should feel excited!

## Regression: 

```{r,echo=FALSE}
pr.var=pca$sdev ^2
pve=pr.var/sum(pr.var)
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='l')
```

Observe that the first two principal components explain nearly 60% of the variability in the data. As $M \to 10$, the marginal contribution to variability explained decreases. Our regression model will use cross validation to tune $M$, the number of components as predictors.

```{r,echo=FALSE}
library(pls)
set.seed(47)
train=sample(1:nrow(pokemondata),nrow(pokemondata)*2/3)
pcr.fit = pcr(cpnew~.,data=pokemondata[train,],scale=TRUE,validation="CV")
summary(pcr.fit)
```


The metric of cross-validation is $root\_mean\_squared\_error$. Ideally, we want to pick $M$ where the CV score is minimized. 

```{r,echo=FALSE}
validationplot(pcr.fit,val.type="MSEP")
```

We compute the test MSE as follows using $M=9$ components:
```{r}
pcr.pred=predict(pcr.fit,pokemondata[-train,],ncomp=9) 
mean((pcr.pred-pokemondata[-train,]$cpnew)^2)
```

\subsubsection{Interpretation}

\section*{Summary Report}

